"""
RSS Crawler ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u v√† s·ª≠a l·ªói
H·ªó tr·ª£ VietStock v√† StockBiz
"""

import requests
import feedparser
import pandas as pd
from datetime import datetime, timedelta
import time
import re
import logging
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import json

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class RSSConfig:
    """Configuration cho RSS Crawler"""
    base_url: str
    rss_page_url: str
    name: str
    headers: Dict[str, str] = field(default_factory=lambda: {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    timeout: int = 30
    max_retries: int = 3
    delay_between_requests: float = 1.0

@dataclass
class Article:
    """Data class cho b√†i vi·∫øt"""
    title: str
    link: str
    description: str
    pub_date: str
    category: str
    section: str
    source: str
    crawl_time: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'title': self.title,
            'link': self.link,
            'description': self.description,
            'pub_date': self.pub_date,
            'category': self.category,
            'section': self.section,
            'source': self.source,
            'crawl_time': self.crawl_time
        }

class BaseRSSCrawler(ABC):
    """Base class cho RSS Crawler"""
    
    def __init__(self, config: RSSConfig):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update(config.headers)
        self.rss_feeds: List[Dict[str, str]] = []
        self.articles: List[Article] = []
    
    @abstractmethod
    def find_rss_links(self) -> List[Dict[str, str]]:
        """T√¨m t·∫•t c·∫£ RSS links t·ª´ trang web"""
        pass
    
    @abstractmethod
    def get_section_from_url(self, url: str) -> str:
        """L·∫•y section t·ª´ URL"""
        pass
    
    def clean_description(self, description: str) -> str:
        """L√†m s·∫°ch m√¥ t·∫£ b√†i vi·∫øt"""
        if not description:
            return ""
        
        # B·ªè HTML tags
        description = re.sub(r'<[^>]+>', '', description)
        # B·ªè kho·∫£ng tr·∫Øng th·ª´a
        description = ' '.join(description.split())
        return description.strip()
    
    def parse_date(self, date_str: str) -> str:
        """Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y"""
        if not date_str:
            return ""
        
        try:
            # Th·ª≠ parse v·ªõi feedparser
            import email.utils
            parsed_time = email.utils.parsedate_tz(date_str)
            if parsed_time:
                dt = datetime(*parsed_time[:6])
                return dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            pass
        
        return date_str
    
    def parse_rss_feed(self, rss_url: str, category: str, section: str) -> List[Article]:
        """Parse m·ªôt RSS feed v·ªõi retry logic"""
        for attempt in range(self.config.max_retries):
            try:
                logger.info(f"Crawling RSS: {category} (attempt {attempt + 1})")
                
                # S·ª≠ d·ª•ng requests ƒë·ªÉ get RSS v·ªõi timeout
                response = self.session.get(rss_url, timeout=self.config.timeout)
                response.raise_for_status()
                
                # Ki·ªÉm tra content type
                content_type = response.headers.get('content-type', '').lower()
                if 'xml' not in content_type and 'rss' not in content_type and 'text' not in content_type:
                    logger.warning(f"Unexpected content type for {category}: {content_type}")
                
                # Parse v·ªõi feedparser
                feed = feedparser.parse(response.content)
                
                if not feed.entries:
                    logger.warning(f"No articles found in RSS: {category}")
                    return []
                
                articles = []
                for entry in feed.entries:
                    try:
                        title = getattr(entry, 'title', '').strip()
                        link = getattr(entry, 'link', '').strip()
                        description = getattr(entry, 'description', '') or getattr(entry, 'summary', '')
                        pub_date = getattr(entry, 'published', '') or getattr(entry, 'updated', '')
                        
                        # Validate required fields
                        if not title or not link:
                            continue
                        
                        article = Article(
                            title=title,
                            link=link,
                            description=self.clean_description(description),
                            pub_date=self.parse_date(pub_date),
                            category=category,
                            section=section,
                            source=self.config.name,
                            crawl_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        )
                        
                        articles.append(article)
                        
                    except Exception as e:
                        logger.error(f"Error parsing entry in RSS {category}: {e}")
                        continue
                
                logger.info(f"Successfully crawled {len(articles)} articles from {category}")
                return articles
                
            except requests.exceptions.HTTPError as e:
                if e.response and e.response.status_code == 404:
                    logger.error(f"RSS feed not found (404): {category} - {rss_url}")
                    return []  # Kh√¥ng retry cho 404
                else:
                    logger.error(f"HTTP error for RSS {category} (attempt {attempt + 1}): {e}")
            except requests.exceptions.Timeout:
                logger.error(f"Timeout for RSS {category} (attempt {attempt + 1})")
            except Exception as e:
                logger.error(f"Error crawling RSS {category} (attempt {attempt + 1}): {e}")
            
            # Retry logic
            if attempt < self.config.max_retries - 1:
                sleep_time = 2 ** attempt
                logger.info(f"Retrying in {sleep_time} seconds...")
                time.sleep(sleep_time)  # Exponential backoff
            else:
                logger.error(f"Failed to crawl RSS {category} after {self.config.max_retries} attempts")
                
        return []
    
    def crawl_all_feeds(self, max_workers: int = 3) -> List[Article]:
        """Crawl t·∫•t c·∫£ RSS feeds v·ªõi threading"""
        if not self.rss_feeds:
            logger.info("Finding RSS feeds...")
            self.find_rss_links()
        
        if not self.rss_feeds:
            logger.error("No RSS feeds found!")
            return []
        
        all_articles = []
        
        # Crawl tu·∫ßn t·ª± ƒë·ªÉ tr√°nh spam server
        for i, feed in enumerate(self.rss_feeds):
            try:
                logger.info(f"[{i+1}/{len(self.rss_feeds)}] Processing: {feed['category']}")
                articles = self.parse_rss_feed(feed['url'], feed['category'], feed['section'])
                all_articles.extend(articles)
                
                # Ngh·ªâ m·ªôt ch√∫t ƒë·ªÉ tr√°nh spam server
                time.sleep(self.config.delay_between_requests)
                
            except Exception as e:
                logger.error(f"Error in feed {feed['category']}: {e}")
        
        self.articles = all_articles
        logger.info(f"Total crawled {len(all_articles)} articles from all RSS feeds")
        return all_articles

class VietStockRSSCrawler(BaseRSSCrawler):
    """RSS Crawler cho VietStock"""
    
    def find_rss_links(self) -> List[Dict[str, str]]:
        """T√¨m t·∫•t c·∫£ c√°c link RSS t·ª´ trang RSS c·ªßa VietStock"""
        try:
            response = self.session.get(self.config.rss_page_url, timeout=self.config.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            rss_links = []
            for link in soup.find_all('a', href=True):
                try:
                    href = link['href'] if link.has_attr('href') else ''
                    href = str(href) if href else ''
                    if href and ('.rss' in href or '.ashx' in href):
                        full_url = urljoin(self.config.base_url, href)
                        category = link.get_text(strip=True)
                        
                        if category and full_url:  # Validate
                            rss_links.append({
                                'url': full_url,
                                'category': category,
                                'section': self.get_section_from_url(href)
                            })
                            
                except Exception as e:
                    logger.error(f"Error processing link: {e}")
                    continue
            
            self.rss_feeds = rss_links
            logger.info(f"Found {len(rss_links)} RSS feeds from VietStock")
            return rss_links
            
        except Exception as e:
            logger.error(f"Error finding RSS links: {e}")
            return []
    
    def get_section_from_url(self, url: str) -> str:
        """L·∫•y section t·ª´ URL RSS"""
        sections = {
            'chung-khoan': 'Ch·ª©ng kho√°n',
            'doanh-nghiep': 'Doanh nghi·ªáp', 
            'bat-dong-san': 'B·∫•t ƒë·ªông s·∫£n',
            'hang-hoa': 'H√†ng h√≥a',
            'tai-chinh': 'T√†i ch√≠nh',
            'kinh-te': 'Kinh t·∫ø',
            'the-gioi': 'Th·∫ø gi·ªõi',
            'dong-duong': 'ƒê√¥ng D∆∞∆°ng',
            'tai-chinh-ca-nhan': 'T√†i ch√≠nh c√° nh√¢n',
            'nhan-dinh-phan-tich': 'Ph√¢n t√≠ch'
        }
        
        url_lower = url.lower()
        for key, value in sections.items():
            if key in url_lower:
                return value
        return 'Kh√°c'

class StockBizRSSCrawler(BaseRSSCrawler):
    """RSS Crawler cho StockBiz"""
    
    def find_rss_links(self) -> List[Dict[str, str]]:
        """T√¨m t·∫•t c·∫£ c√°c link RSS t·ª´ trang RSS c·ªßa VietStock"""
        try:
            response = self.session.get(self.config.rss_page_url, timeout=self.config.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            rss_links = []
            for link in soup.find_all('a', href=True):
                try:
                    href = link['href'] if link.has_attr('href') else ''
                    href = str(href) if href else ''
                    if href and ('.rss' in href or '.ashx' in href):
                        full_url = urljoin(self.config.base_url, href)
                        category = link.get_text(strip=True)
                        
                        if category and full_url:  # Validate
                            rss_links.append({
                                'url': full_url,
                                'category': category,
                                'section': self.get_section_from_url(href)
                            })
                            
                except Exception as e:
                    logger.error(f"Error processing link: {e}")
                    continue
            
            self.rss_feeds = rss_links
            logger.info(f"Found {len(rss_links)} RSS feeds from VietStock")
            return rss_links
            
        except Exception as e:
            logger.error(f"Error finding RSS links: {e}")
            return []
    
    def get_section_from_url(self, url: str) -> str:
        """L·∫•y section t·ª´ URL RSS"""
        sections = {
            'market': 'Th·ªã tr∆∞·ªùng',
            'stock': 'C·ªï phi·∫øu', 
            'finance': 'T√†i ch√≠nh',
            'economy': 'Kinh t·∫ø',
            'business': 'Kinh doanh',
            'investment': 'ƒê·∫ßu t∆∞',
            'news': 'Tin t·ª©c',
            'analysis': 'Ph√¢n t√≠ch'
        }
        
        url_lower = url.lower()
        for key, value in sections.items():
            if key in url_lower:
                return value
        return 'Tin t·ª©c'

class RSSCrawlerFactory:
    """Factory ƒë·ªÉ t·∫°o c√°c RSS Crawler"""
    
    @staticmethod
    def create_crawler(source: str) -> BaseRSSCrawler:
        """T·∫°o crawler based on source"""
        if source.lower() == 'vietstock':
            config = RSSConfig(
                base_url="https://vietstock.vn",
                rss_page_url="https://vietstock.vn/rss",
                name="VietStock"
            )
            return VietStockRSSCrawler(config)
        
        elif source.lower() == 'stockbiz':
            config = RSSConfig(
                base_url="http://en.stockbiz.vn",
                rss_page_url="http://en.stockbiz.vn/Rss.aspx",
                name="StockBiz",
                timeout=15,
                max_retries=2
            )
            return StockBizRSSCrawler(config)
        
        else:
            raise ValueError(f"Unsupported source: {source}")

class MultiSourceRSSCrawler:
    """Crawler t·ªïng h·ª£p t·ª´ nhi·ªÅu ngu·ªìn"""
    
    def __init__(self, sources: List[str]):
        self.sources = sources
        self.crawlers = [RSSCrawlerFactory.create_crawler(source) for source in sources]
        self.all_articles: List[Article] = []
    
    def crawl_all_sources(self) -> List[Article]:
        """Crawl t·ª´ t·∫•t c·∫£ c√°c ngu·ªìn"""
        all_articles = []
        
        for crawler in self.crawlers:
            try:
                logger.info(f"Starting crawl for {crawler.config.name}")
                articles = crawler.crawl_all_feeds()
                all_articles.extend(articles)
                logger.info(f"Completed {crawler.config.name}: {len(articles)} articles")
            except Exception as e:
                logger.error(f"Error crawling {crawler.config.name}: {e}")
        
        self.all_articles = all_articles
        return all_articles
    
    def save_to_csv(self, filename: str = 'merged_rss_data.csv') -> None:
        """L∆∞u t·∫•t c·∫£ d·ªØ li·ªáu v√†o file CSV"""
        if not self.all_articles:
            logger.warning("No articles to save!")
            return
        
        try:
            # Convert articles to dict
            articles_data = [article.to_dict() for article in self.all_articles]
            df = pd.DataFrame(articles_data)
            
            # Remove duplicates based on link
            initial_count = len(df)
            df = df.drop_duplicates(subset=['link'], keep='first')
            duplicate_count = initial_count - len(df)
            
            if duplicate_count > 0:
                logger.info(f"Removed {duplicate_count} duplicate articles")
            
            # Sort by crawl_time
            df = df.sort_values('crawl_time', ascending=False)
            
            # Save to CSV
            df.to_csv('./data/rss_news/' + filename, index=False, encoding='utf-8')
            logger.info(f"Saved {len(df)} articles to {filename}")
            
            # Display statistics
            self._display_statistics(df)
            
        except Exception as e:
            logger.error(f"Error saving to CSV: {e}")
    
    def _display_statistics(self, df: pd.DataFrame) -> None:
        """Hi·ªÉn th·ªã th·ªëng k√™"""
        print(f"\nüìä TH·ªêNG K√ä T·ªîNG H·ª¢P:")
        print(f"T·ªïng s·ªë b√†i vi·∫øt: {len(df)}")
        
        print(f"\nüìà Theo ngu·ªìn:")
        source_stats = df['source'].value_counts()
        for source, count in source_stats.items():
            print(f"- {source}: {count} b√†i vi·∫øt")
        
        print(f"\nüìÇ Theo chuy√™n m·ª•c:")
        section_stats = df['section'].value_counts()
        for section, count in section_stats.head(10).items():
            print(f"- {section}: {count} b√†i vi·∫øt")
        
        print(f"\nüóÇÔ∏è Theo danh m·ª•c:")
        category_stats = df['category'].value_counts()
        for category, count in category_stats.head(10).items():
            print(f"- {category}: {count} b√†i vi·∫øt")
    
    def save_feeds_info(self, filename: str = 'all_rss_feeds.csv') -> None:
        """L∆∞u th√¥ng tin t·∫•t c·∫£ RSS feeds"""
        all_feeds = []
        
        for crawler in self.crawlers:
            for feed in crawler.rss_feeds:
                feed_info = feed.copy()
                feed_info['source'] = crawler.config.name
                all_feeds.append(feed_info)
        
        if all_feeds:
            df = pd.DataFrame(all_feeds)
            df.to_csv(filename, index=False, encoding='utf-8')
            logger.info(f"Saved {len(all_feeds)} RSS feeds info to {filename}")

def main():
    """H√†m main ƒë·ªÉ ch·∫°y crawler t·ªïng h·ª£p"""
    print("üöÄ B·∫Øt ƒë·∫ßu crawl RSS feeds t·ª´ nhi·ªÅu ngu·ªìn...")
    
    # Kh·ªüi t·∫°o crawler v·ªõi nhi·ªÅu ngu·ªìn
    sources = ['stockbiz', 'vietstock']
    crawler = MultiSourceRSSCrawler(sources)
    
    try:
        # Crawl t·ª´ t·∫•t c·∫£ ngu·ªìn
        print("\nüì∞ ƒêang crawl t·ª´ t·∫•t c·∫£ ngu·ªìn...")
        articles = crawler.crawl_all_sources()
        
        if not articles:
            print("‚ùå Kh√¥ng crawl ƒë∆∞·ª£c b√†i vi·∫øt n√†o!")
            return
        
        # L∆∞u th√¥ng tin RSS feeds
        print("\nüíæ L∆∞u th√¥ng tin RSS feeds...")
        crawler.save_feeds_info()
        
        # L∆∞u d·ªØ li·ªáu merge v√†o CSV
        print("\nüíæ L∆∞u d·ªØ li·ªáu merge v√†o CSV...")
        crawler.save_to_csv()
        
        print("\n‚úÖ Ho√†n th√†nh crawl RSS feeds t·ª´ t·∫•t c·∫£ ngu·ªìn!")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è ƒê√£ d·ª´ng crawling...")
    except Exception as e:
        logger.error(f"Error in main: {e}")
        print(f"‚ùå L·ªói: {e}")

if __name__ == "__main__":
    main()
