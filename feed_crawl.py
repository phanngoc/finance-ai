import requests
import feedparser
import pandas as pd
from datetime import datetime
import time
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional

class VietStockRSSCrawler:
    def __init__(self):
        self.base_url = "https://vietstock.vn"
        self.rss_page_url = "https://vietstock.vn/rss"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.rss_feeds: List[Dict[str, str]] = []
        self.articles: List[Dict[str, Any]] = []
        
    def find_rss_links(self) -> List[Dict[str, str]]:
        """TÃ¬m táº¥t cáº£ cÃ¡c link RSS tá»« trang RSS cá»§a VietStock"""
        try:
            response = requests.get(self.rss_page_url, headers=self.headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # TÃ¬m táº¥t cáº£ cÃ¡c link cÃ³ chá»©a '.rss'
            rss_links = []
            for link in soup.find_all('a', href=True):
                try:
                    # Sá»­ dá»¥ng dictionary-style access
                    href = link['href'] if link.has_attr('href') else ''
                    if href and '.rss' in href:
                        # Chuyá»ƒn Ä‘á»•i link tÆ°Æ¡ng Ä‘á»‘i thÃ nh link tuyá»‡t Ä‘á»‘i
                        full_url = urljoin(self.base_url, href)
                        category = link.get_text(strip=True)
                        rss_links.append({
                            'url': full_url,
                            'category': category,
                            'section': self.get_section_from_url(href)
                        })
                except Exception as e:
                    print(f"Lá»—i khi xá»­ lÃ½ link: {e}")
                    continue
            
            self.rss_feeds = rss_links
            print(f"ÄÃ£ tÃ¬m tháº¥y {len(rss_links)} RSS feeds:")
            for feed in rss_links:
                print(f"- {feed['category']}: {feed['url']}")
            
            return rss_links
            
        except Exception as e:
            print(f"Lá»—i khi tÃ¬m RSS links: {e}")
            return []
    
    def get_section_from_url(self, url: str) -> str:
        """Láº¥y section tá»« URL RSS"""
        sections = {
            'chung-khoan': 'Chá»©ng khoÃ¡n',
            'doanh-nghiep': 'Doanh nghiá»‡p',
            'bat-dong-san': 'Báº¥t Ä‘á»™ng sáº£n',
            'hang-hoa': 'HÃ ng hÃ³a',
            'tai-chinh': 'TÃ i chÃ­nh',
            'kinh-te': 'Kinh táº¿',
            'the-gioi': 'Tháº¿ giá»›i',
            'dong-duong': 'ÄÃ´ng DÆ°Æ¡ng',
            'tai-chinh-ca-nhan': 'TÃ i chÃ­nh cÃ¡ nhÃ¢n',
            'nhan-dinh-phan-tich': 'PhÃ¢n tÃ­ch'
        }
        
        for key, value in sections.items():
            if key in url:
                return value
        return 'KhÃ¡c'
    
    def parse_rss_feed(self, rss_url: str, category: str, section: str) -> List[Dict[str, Any]]:
        """Parse má»™t RSS feed vÃ  trÃ­ch xuáº¥t thÃ´ng tin bÃ i viáº¿t"""
        try:
            print(f"Äang crawl RSS: {category}")
            
            # Sá»­ dá»¥ng feedparser Ä‘á»ƒ parse RSS
            feed = feedparser.parse(rss_url)
            
            if not feed.entries:
                print(f"KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o trong RSS: {category}")
                return []
            
            feed_articles = []
            for entry in feed.entries:
                try:
                    title = getattr(entry, 'title', '')
                    link = getattr(entry, 'link', '')
                    description = getattr(entry, 'description', '') or getattr(entry, 'summary', '')
                    pub_date = getattr(entry, 'published', '') or getattr(entry, 'updated', '')
                    
                    # LÃ m sáº¡ch description (bá» HTML tags)
                    if description:
                        description = re.sub(r'<[^>]+>', '', description).strip()
                    
                    article = {
                        'title': title.strip() if title else '',
                        'link': link.strip() if link else '',
                        'description': description,
                        'pub_date': pub_date,
                        'category': category,
                        'section': section,
                        'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    feed_articles.append(article)
                    
                except Exception as e:
                    print(f"Lá»—i khi parse entry trong RSS {category}: {e}")
                    continue
            
            print(f"ÄÃ£ crawl {len(feed_articles)} bÃ i viáº¿t tá»« {category}")
            return feed_articles
            
        except Exception as e:
            print(f"Lá»—i khi crawl RSS {category}: {e}")
            return []
    
    def crawl_all_feeds(self) -> List[Dict[str, Any]]:
        """Crawl táº¥t cáº£ RSS feeds"""
        if not self.rss_feeds:
            print("ChÆ°a tÃ¬m tháº¥y RSS feeds. Äang tÃ¬m kiáº¿m...")
            self.find_rss_links()
        
        all_articles = []
        
        for i, feed in enumerate(self.rss_feeds):
            try:
                print(f"[{i+1}/{len(self.rss_feeds)}] Crawling: {feed['category']}")
                articles = self.parse_rss_feed(feed['url'], feed['category'], feed['section'])
                all_articles.extend(articles)
                
                # Nghá»‰ má»™t chÃºt Ä‘á»ƒ trÃ¡nh spam server
                time.sleep(1)
                
            except Exception as e:
                print(f"Lá»—i khi crawl feed {feed['category']}: {e}")
                continue
        
        self.articles = all_articles
        print(f"\nTá»•ng cá»™ng Ä‘Ã£ crawl {len(all_articles)} bÃ i viáº¿t tá»« táº¥t cáº£ RSS feeds")
        return all_articles
    
    def save_to_csv(self, filename: str = 'vietstock_rss_data.csv') -> None:
        """LÆ°u dá»¯ liá»‡u vÃ o file CSV"""
        if not self.articles:
            print("KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u!")
            return
        
        try:
            df = pd.DataFrame(self.articles)
            
            # Sáº¯p xáº¿p theo thá»i gian publish (náº¿u cÃ³)
            if 'pub_date' in df.columns and not df['pub_date'].isna().all():
                df = df.sort_values('pub_date', ascending=False)
            
            # LÆ°u vÃ o CSV
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"ÄÃ£ lÆ°u {len(self.articles)} bÃ i viáº¿t vÃ o file {filename}")
            
            # Hiá»ƒn thá»‹ thá»‘ng kÃª
            print("\nThá»‘ng kÃª theo danh má»¥c:")
            category_stats = df['category'].value_counts()
            for category, count in category_stats.items():
                print(f"- {category}: {count} bÃ i viáº¿t")
            
            print("\nThá»‘ng kÃª theo chuyÃªn má»¥c:")
            section_stats = df['section'].value_counts()
            for section, count in section_stats.items():
                print(f"- {section}: {count} bÃ i viáº¿t")
                
        except Exception as e:
            print(f"Lá»—i khi lÆ°u file CSV: {e}")
    
    def save_rss_feeds_info(self, filename: str = 'vietstock_rss_feeds.csv') -> None:
        """LÆ°u thÃ´ng tin cÃ¡c RSS feeds vÃ o file CSV"""
        if not self.rss_feeds:
            print("KhÃ´ng cÃ³ thÃ´ng tin RSS feeds Ä‘á»ƒ lÆ°u!")
            return
        
        try:
            df = pd.DataFrame(self.rss_feeds)
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"ÄÃ£ lÆ°u thÃ´ng tin {len(self.rss_feeds)} RSS feeds vÃ o file {filename}")
            
        except Exception as e:
            print(f"Lá»—i khi lÆ°u file RSS feeds: {e}")

def main():
    """HÃ m main Ä‘á»ƒ cháº¡y crawler"""
    print("ğŸš€ Báº¯t Ä‘áº§u crawl RSS feeds tá»« VietStock...")
    
    crawler = VietStockRSSCrawler()
    
    # BÆ°á»›c 1: TÃ¬m táº¥t cáº£ RSS links
    print("\nğŸ“¡ Äang tÃ¬m kiáº¿m RSS feeds...")
    rss_links = crawler.find_rss_links()
    
    if not rss_links:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y RSS feeds nÃ o!")
        return
    
    # BÆ°á»›c 2: LÆ°u thÃ´ng tin RSS feeds
    print("\nğŸ’¾ LÆ°u thÃ´ng tin RSS feeds...")
    crawler.save_rss_feeds_info()
    
    # BÆ°á»›c 3: Crawl táº¥t cáº£ RSS feeds
    print("\nğŸ“° Äang crawl táº¥t cáº£ RSS feeds...")
    articles = crawler.crawl_all_feeds()
    
    if not articles:
        print("âŒ KhÃ´ng crawl Ä‘Æ°á»£c bÃ i viáº¿t nÃ o!")
        return
    
    # BÆ°á»›c 4: LÆ°u dá»¯ liá»‡u vÃ o CSV
    print("\nğŸ’¾ LÆ°u dá»¯ liá»‡u vÃ o CSV...")
    crawler.save_to_csv()
    
    print("\nâœ… HoÃ n thÃ nh crawl RSS feeds tá»« VietStock!")

if __name__ == "__main__":
    main()